{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import experiment_runner\n",
    "import generate_synthetic_data\n",
    "import neural_network\n",
    "from experiment_runner import ExperimentRunner\n",
    "from generate_synthetic_data import GenerateSyntheticData\n",
    "from experiment_runner import ExperimentRunner\n",
    "importlib.reload(experiment_runner)\n",
    "importlib.reload(generate_synthetic_data)\n",
    "importlib.reload(neural_network)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import preprocessor\n",
    "from preprocess_data import PreprocessData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Import the files\n",
    "2. Transform all their features\n",
    "3. Use the experiment runner and the generate synthetic data class to get info for the experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyBjGoZ2striQdnjR16SmGoviRKeEcGc2NU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0075\n",
      "Epoch [20/100], Loss: 0.0038\n",
      "Epoch [30/100], Loss: 0.0023\n",
      "Epoch [40/100], Loss: 0.0015\n",
      "Epoch [50/100], Loss: 0.0011\n",
      "Epoch [60/100], Loss: 0.0008\n",
      "Epoch [70/100], Loss: 0.0006\n",
      "Epoch [80/100], Loss: 0.0004\n",
      "Epoch [90/100], Loss: 0.0003\n",
      "Epoch [100/100], Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\siddharth\\programming\\school\\am226-final-proj\\experiment_runner.py:26: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  characteristics['mean'] = full_data.mean().to_dict()\n",
      "c:\\siddharth\\programming\\school\\am226-final-proj\\experiment_runner.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  characteristics['std'] = full_data.std().to_dict()\n",
      "c:\\siddharth\\programming\\school\\am226-final-proj\\experiment_runner.py:30: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  characteristics['median'] = full_data.median().to_dict()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Honda Activa 125'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m experiment_runner\u001b[38;5;241m.\u001b[39mtrain_network(train_data)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Benchmark network\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     39\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubset_proportion\u001b[39m\u001b[38;5;124m\"\u001b[39m: prop,\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m : n,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse\n\u001b[0;32m     46\u001b[0m })\n",
      "File \u001b[1;32mc:\\siddharth\\programming\\school\\am226-final-proj\\experiment_runner.py:55\u001b[0m, in \u001b[0;36mExperimentRunner.benchmark_network\u001b[1;34m(self, test_data)\u001b[0m\n\u001b[0;32m     52\u001b[0m targets \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_column]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Get the predictions\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Benchmark using the mean squared error\u001b[39;00m\n\u001b[0;32m     58\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(targets, predictions)\n",
      "File \u001b[1;32mc:\\siddharth\\programming\\school\\am226-final-proj\\neural_network.py:153\u001b[0m, in \u001b[0;36mNeuralNetworkWrapper.predict\u001b[1;34m(self, inputs, column_names)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03mInputs are the data we actually want to predict\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Convert inputs to numerical format\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\siddharth\\programming\\school\\am226-final-proj\\neural_network.py:116\u001b[0m, in \u001b[0;36mNeuralNetworkWrapper._convert_to_numeric\u001b[1;34m(self, data, column_names)\u001b[0m\n\u001b[0;32m    114\u001b[0m         unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column)\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmappings[col_name] \u001b[38;5;241m=\u001b[39m {value: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_values)}\n\u001b[1;32m--> 116\u001b[0m     mapped_column \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmappings[col_name][value] \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column])\n\u001b[0;32m    117\u001b[0m     processed_data\u001b[38;5;241m.\u001b[39mappend(mapped_column)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\siddharth\\programming\\school\\am226-final-proj\\neural_network.py:116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    114\u001b[0m         unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column)\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmappings[col_name] \u001b[38;5;241m=\u001b[39m {value: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_values)}\n\u001b[1;32m--> 116\u001b[0m     mapped_column \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmappings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column])\n\u001b[0;32m    117\u001b[0m     processed_data\u001b[38;5;241m.\u001b[39mappend(mapped_column)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Honda Activa 125'"
     ]
    }
   ],
   "source": [
    "# Load dataset (replace with your dataset)\n",
    "dataset = pd.read_csv('car.csv') \n",
    "target_column = 'Transmission' \n",
    "#subset_sizes = [0.01, 0.1, 0.3, 0.5, 0.7, 1.0]  # Proportions of the dataset\n",
    "\n",
    "subset_sizes = [0.01]\n",
    "\n",
    "# Initialize synthetic data generator and experiment runner\n",
    "synthetic_data_generator = GenerateSyntheticData(API_KEY)\n",
    "experiment_runner = ExperimentRunner(dataset, target_column)\n",
    "\n",
    "# Loop through subset sizes\n",
    "results = []\n",
    "for prop in subset_sizes:\n",
    "    # Take subset of the data\n",
    "    subset = dataset.sample(frac=prop, random_state=42)\n",
    "    \n",
    "    # Compute subset characteristics (dimensions, variance, skewness)\n",
    "    characteristics = experiment_runner.compute_characteristics(subset)\n",
    "    \n",
    "    # Generate synthetic data using subset\n",
    "    synthetic_data = synthetic_data_generator.predict(10,subset)\n",
    "    print(synthetic_data)\n",
    "    \n",
    "    # Combine real and synthetic data for training\n",
    "    # combined_data = pd.concat([subset, synthetic_data])\n",
    "    combined_data = subset\n",
    "\n",
    "    # Take combined data and alter it\n",
    "\n",
    "\n",
    "    # Split combined data into train/test\n",
    "    train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=226)\n",
    "    \n",
    "    # Train neural network\n",
    "    experiment_runner.train_network(train_data)\n",
    "    \n",
    "    # Benchmark network\n",
    "    mse = experiment_runner.benchmark_network(test_data)\n",
    "    \n",
    "    # Save results\n",
    "    results.append({\n",
    "        \"subset_proportion\": prop,\n",
    "        \"generated_rows\" : n,\n",
    "        \"target_column\": target_column,\n",
    "        \"subset_characteristics\": subset_characteristics,\n",
    "        \"generated_characteristics\" : generated_characteristics,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Subset Proportion: {result['subset_proportion']}\")\n",
    "    print(f\"Characteristics: {result['characteristics']}\")\n",
    "    print(f\"Mean Squared Error: {result['mse']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
