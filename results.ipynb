{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "import experiment_runner\n",
    "import generate_synthetic_data\n",
    "import neural_network\n",
    "from generate_synthetic_data import GenerateSyntheticData\n",
    "from experiment_runner import ExperimentRunner\n",
    "importlib.reload(experiment_runner)\n",
    "importlib.reload(generate_synthetic_data)\n",
    "importlib.reload(neural_network)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import preprocessor\n",
    "from preprocess_data import PreprocessData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Import the files\n",
    "2. Transform all their features\n",
    "3. Use the experiment runner and the generate synthetic data class to get info for the experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyC14K1XMc5q5wOmHKzuLH5zWb_BED8SrkQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "- A single run-through an experiment should work now\n",
    "    - Troubleshoot preprocessing method\n",
    "    - Clarify benchmarking method (or how we pass data to it) so that it's taken on out-of-sample data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([232])) that is different to the input size (torch.Size([232, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 33.3049\n",
      "Epoch [20/100], Loss: 27.3188\n",
      "Epoch [30/100], Loss: 25.9794\n",
      "Epoch [40/100], Loss: 25.6717\n",
      "Epoch [50/100], Loss: 25.5565\n",
      "Epoch [60/100], Loss: 25.5060\n",
      "Epoch [70/100], Loss: 25.4813\n",
      "Epoch [80/100], Loss: 25.4681\n",
      "Epoch [90/100], Loss: 25.4603\n",
      "Epoch [100/100], Loss: 25.4551\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 36.4596\n",
      "Epoch [20/100], Loss: 29.9154\n",
      "Epoch [30/100], Loss: 28.3614\n",
      "Epoch [40/100], Loss: 28.0556\n",
      "Epoch [50/100], Loss: 27.9779\n",
      "Epoch [60/100], Loss: 27.9470\n",
      "Epoch [70/100], Loss: 27.9302\n",
      "Epoch [80/100], Loss: 27.9188\n",
      "Epoch [90/100], Loss: 27.9108\n",
      "Epoch [100/100], Loss: 27.9046\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 38.3105\n",
      "Epoch [20/100], Loss: 30.8920\n",
      "Epoch [30/100], Loss: 29.4419\n",
      "Epoch [40/100], Loss: 29.2344\n",
      "Epoch [50/100], Loss: 29.1839\n",
      "Epoch [60/100], Loss: 29.1623\n",
      "Epoch [70/100], Loss: 29.1499\n",
      "Epoch [80/100], Loss: 29.1416\n",
      "Epoch [90/100], Loss: 29.1354\n",
      "Epoch [100/100], Loss: 29.1306\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([232])) that is different to the input size (torch.Size([232, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.1396\n",
      "Epoch [20/100], Loss: 28.7027\n",
      "Epoch [30/100], Loss: 27.2042\n",
      "Epoch [40/100], Loss: 26.8968\n",
      "Epoch [50/100], Loss: 26.8180\n",
      "Epoch [60/100], Loss: 26.7912\n",
      "Epoch [70/100], Loss: 26.7789\n",
      "Epoch [80/100], Loss: 26.7715\n",
      "Epoch [90/100], Loss: 26.7659\n",
      "Epoch [100/100], Loss: 26.7614\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([232])) that is different to the input size (torch.Size([232, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 37.0163\n",
      "Epoch [20/100], Loss: 30.9977\n",
      "Epoch [30/100], Loss: 29.7149\n",
      "Epoch [40/100], Loss: 29.5015\n",
      "Epoch [50/100], Loss: 29.4400\n",
      "Epoch [60/100], Loss: 29.4112\n",
      "Epoch [70/100], Loss: 29.3948\n",
      "Epoch [80/100], Loss: 29.3842\n",
      "Epoch [90/100], Loss: 29.3767\n",
      "Epoch [100/100], Loss: 29.3710\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 39.4555\n",
      "Epoch [20/100], Loss: 31.1957\n",
      "Epoch [30/100], Loss: 28.9819\n",
      "Epoch [40/100], Loss: 28.5556\n",
      "Epoch [50/100], Loss: 28.4459\n",
      "Epoch [60/100], Loss: 28.4054\n",
      "Epoch [70/100], Loss: 28.3864\n",
      "Epoch [80/100], Loss: 28.3753\n",
      "Epoch [90/100], Loss: 28.3678\n",
      "Epoch [100/100], Loss: 28.3623\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([232])) that is different to the input size (torch.Size([232, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.1014\n",
      "Epoch [20/100], Loss: 29.1350\n",
      "Epoch [30/100], Loss: 27.9846\n",
      "Epoch [40/100], Loss: 27.8183\n",
      "Epoch [50/100], Loss: 27.7772\n",
      "Epoch [60/100], Loss: 27.7607\n",
      "Epoch [70/100], Loss: 27.7519\n",
      "Epoch [80/100], Loss: 27.7464\n",
      "Epoch [90/100], Loss: 27.7425\n",
      "Epoch [100/100], Loss: 27.7395\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([232])) that is different to the input size (torch.Size([232, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 34.6759\n",
      "Epoch [20/100], Loss: 28.2536\n",
      "Epoch [30/100], Loss: 26.2182\n",
      "Epoch [40/100], Loss: 25.7125\n",
      "Epoch [50/100], Loss: 25.5536\n",
      "Epoch [60/100], Loss: 25.4905\n",
      "Epoch [70/100], Loss: 25.4596\n",
      "Epoch [80/100], Loss: 25.4422\n",
      "Epoch [90/100], Loss: 25.4310\n",
      "Epoch [100/100], Loss: 25.4230\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 36.3605\n",
      "Epoch [20/100], Loss: 28.1455\n",
      "Epoch [30/100], Loss: 26.0580\n",
      "Epoch [40/100], Loss: 25.7568\n",
      "Epoch [50/100], Loss: 25.6921\n",
      "Epoch [60/100], Loss: 25.6672\n",
      "Epoch [70/100], Loss: 25.6543\n",
      "Epoch [80/100], Loss: 25.6462\n",
      "Epoch [90/100], Loss: 25.6403\n",
      "Epoch [100/100], Loss: 25.6354\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.01\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 39.7721\n",
      "Epoch [20/100], Loss: 31.9545\n",
      "Epoch [30/100], Loss: 29.9064\n",
      "Epoch [40/100], Loss: 29.5061\n",
      "Epoch [50/100], Loss: 29.4095\n",
      "Epoch [60/100], Loss: 29.3743\n",
      "Epoch [70/100], Loss: 29.3574\n",
      "Epoch [80/100], Loss: 29.3474\n",
      "Epoch [90/100], Loss: 29.3409\n",
      "Epoch [100/100], Loss: 29.3364\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([278])) that is different to the input size (torch.Size([278, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 42.2434\n",
      "Epoch [20/100], Loss: 32.1422\n",
      "Epoch [30/100], Loss: 30.1058\n",
      "Epoch [40/100], Loss: 29.8882\n",
      "Epoch [50/100], Loss: 29.8391\n",
      "Epoch [60/100], Loss: 29.8179\n",
      "Epoch [70/100], Loss: 29.8060\n",
      "Epoch [80/100], Loss: 29.7982\n",
      "Epoch [90/100], Loss: 29.7925\n",
      "Epoch [100/100], Loss: 29.7880\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([264])) that is different to the input size (torch.Size([264, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.5224\n",
      "Epoch [20/100], Loss: 28.1836\n",
      "Epoch [30/100], Loss: 25.6420\n",
      "Epoch [40/100], Loss: 25.0645\n",
      "Epoch [50/100], Loss: 24.9312\n",
      "Epoch [60/100], Loss: 24.8881\n",
      "Epoch [70/100], Loss: 24.8683\n",
      "Epoch [80/100], Loss: 24.8564\n",
      "Epoch [90/100], Loss: 24.8482\n",
      "Epoch [100/100], Loss: 24.8421\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([303])) that is different to the input size (torch.Size([303, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 37.0402\n",
      "Epoch [20/100], Loss: 28.7791\n",
      "Epoch [30/100], Loss: 26.6634\n",
      "Epoch [40/100], Loss: 26.1113\n",
      "Epoch [50/100], Loss: 25.9522\n",
      "Epoch [60/100], Loss: 25.9006\n",
      "Epoch [70/100], Loss: 25.8789\n",
      "Epoch [80/100], Loss: 25.8667\n",
      "Epoch [90/100], Loss: 25.8582\n",
      "Epoch [100/100], Loss: 25.8516\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([264])) that is different to the input size (torch.Size([264, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 31.8656\n",
      "Epoch [20/100], Loss: 24.9157\n",
      "Epoch [30/100], Loss: 23.0739\n",
      "Epoch [40/100], Loss: 22.6476\n",
      "Epoch [50/100], Loss: 22.5031\n",
      "Epoch [60/100], Loss: 22.4353\n",
      "Epoch [70/100], Loss: 22.3969\n",
      "Epoch [80/100], Loss: 22.3719\n",
      "Epoch [90/100], Loss: 22.3539\n",
      "Epoch [100/100], Loss: 22.3401\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 0\n",
      "Trial # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([264])) that is different to the input size (torch.Size([264, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 34.4513\n",
      "Epoch [20/100], Loss: 27.5839\n",
      "Epoch [30/100], Loss: 25.7544\n",
      "Epoch [40/100], Loss: 25.3906\n",
      "Epoch [50/100], Loss: 25.2814\n",
      "Epoch [60/100], Loss: 25.2327\n",
      "Epoch [70/100], Loss: 25.2057\n",
      "Epoch [80/100], Loss: 25.1891\n",
      "Epoch [90/100], Loss: 25.1776\n",
      "Epoch [100/100], Loss: 25.1695\n",
      "Current experiment\n",
      "Subset size: 0.1\n",
      "Row percentage: 0.5\n",
      "Drop indicator (0 is false): 1\n",
      "Trial # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew': skew(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/experiment_runner.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis': kurtosis(col_data),\n",
      "/Users/elizaknapp/Desktop/Harvard/am_226/am226/neural_network.py:40: RuntimeWarning: invalid value encountered in divide\n",
      "  inputs = (inputs - inputs.mean(axis=0)) / inputs.std(axis=0)\n",
      "/Users/elizaknapp/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([283])) that is different to the input size (torch.Size([283, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: nan\n",
      "Epoch [20/100], Loss: nan\n",
      "Epoch [30/100], Loss: nan\n",
      "Epoch [40/100], Loss: nan\n",
      "Epoch [50/100], Loss: nan\n",
      "Epoch [60/100], Loss: nan\n",
      "Epoch [70/100], Loss: nan\n",
      "Epoch [80/100], Loss: nan\n",
      "Epoch [90/100], Loss: nan\n",
      "Epoch [100/100], Loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial failed because gradient blowup\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Benchmark network\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: prop,\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m : n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse\n\u001b[1;32m     74\u001b[0m     })\n",
      "File \u001b[0;32m~/Desktop/Harvard/am_226/am226/experiment_runner.py:115\u001b[0m, in \u001b[0;36mExperimentRunner.benchmark_network\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m    113\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneural_network\u001b[38;5;241m.\u001b[39mpredict(inputs)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Benchmark using the mean squared error\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mse\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "dataset = pd.read_csv('car.csv') \n",
    "target_column = 'Selling_Price' \n",
    "\n",
    "# These are in fractions\n",
    "subset_sizes = [0.1] \n",
    "# Number of rows to generate\n",
    "# TODO: added 0.01 for testing, change later!!\n",
    "row_sizes =  [0.01, 0.5, 1, 5, 10] # This should be len(dataset) * array number\n",
    "# Number of experiments to run\n",
    "num_trials = 5\n",
    "\n",
    "# Initialize synthetic data generator and experiment runner\n",
    "synthetic_data_generator = GenerateSyntheticData(API_KEY)\n",
    "# Initialize the preprocessor based on the given dataset\n",
    "preprocessor = PreprocessData(dataset, target_column)\n",
    "\n",
    "# Loop through subset sizes\n",
    "results = []\n",
    "for prop in subset_sizes:\n",
    "    for n in row_sizes:\n",
    "        for s in range(2): # This is whether we drop indicator\n",
    "            for i in range(num_trials): \n",
    "                print(\"Current experiment\")\n",
    "                print(f\"Subset size: {prop}\")\n",
    "                print(f\"Row percentage: {n}\")\n",
    "                print(f\"Drop indicator (0 is false): {s}\")\n",
    "                print(f\"Trial # {i}\")\n",
    "                # Take subset of the data\n",
    "                subset = dataset.sample(frac=prop)\n",
    "                # Generate synthetic data using subset + additional information\n",
    "                synthetic_data = synthetic_data_generator.predict(n,len(dataset),subset)\n",
    "                # Combine real and synthetic data for training\n",
    "                dataset['source'] = 0\n",
    "                synthetic_data['source'] = 1\n",
    "\n",
    "                # Create a train test split \n",
    "                # Validate the columns in the generated data... --> make sure the one hot encoding is not different\n",
    "                train_data, test_data = train_test_split(dataset)\n",
    "                combined_data = pd.concat([train_data, synthetic_data])\n",
    "\n",
    "                # Preprocess data\n",
    "                combined_df_processed = preprocessor.preprocess(combined_data)\n",
    "                test_data_processed = preprocessor.preprocess(test_data)\n",
    "                \n",
    "                if s != 0:\n",
    "                    # If we don't check source column\n",
    "                    combined_df_processed = combined_df_processed.drop(columns=['source'])\n",
    "                    test_data_processed = test_data_processed.drop(columns=['source'])\n",
    "\n",
    "                # Initialize an experiment runner\n",
    "                experiment_runner = ExperimentRunner(combined_df_processed, target_column)\n",
    "                # Compute subset characteristics (dimensions, variance, skewness)\n",
    "                subset_characteristics = experiment_runner.compute_characteristics(subset)\n",
    "                # Compute generated data characteristics\n",
    "                generated_characteristics = experiment_runner.compute_characteristics(synthetic_data)\n",
    "                # Train the network and benchmark\n",
    "                success = experiment_runner.train_network(combined_df_processed)\n",
    "                if not success:\n",
    "                    mse = \"Trial failed because gradient blowup\"\n",
    "                else:\n",
    "                    # Benchmark network\n",
    "                    mse = experiment_runner.benchmark_network(test_data_processed)\n",
    "                    # Save results\n",
    "                    results.append({\n",
    "                        \"source\": prop,\n",
    "                        \"generated_rows\" : n,\n",
    "                        \"subset_id\" : i,\n",
    "                        \"indicators\" : s,\n",
    "                        \"target_column\": target_column,\n",
    "                        \"subset_characteristics\": subset_characteristics,\n",
    "                        \"generated_characteristics\" : generated_characteristics,\n",
    "                        \"mse\": mse\n",
    "                    })\n",
    "\n",
    "# Write results to a json\n",
    "file_path = \"results.json\"\n",
    "\n",
    "# Write the list of dictionaries to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False ==combined_df_processed.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for visualizations\n",
    "- GENERAL IDEA: See if there's a relationship between proportion of \"realness\" ((prop * len(df)) / (n + prop * len(df))) and preservation of characteristics (average of difference between subset characteristics and generated characteristics) and mse\n",
    "    - Are there certain characteristics (mean, variance, etc.) that are preserved better on average by feature? What properties about the feature make it the case?\n",
    "- Coolest graph would be something like proportion of \"realness\" on x-axis and mse and then two lines corresponding to with and without-indicators\n",
    "    - Could be super cool to see something besides \"indicators always beat out without indicators\" - something unintuitive would be sick\n",
    "- Write down any other ideas!\n",
    "- Just a bunch of facet grids could be cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights on how much the model uses the indicators as information\n",
    "- also try with and without indicators and compare performance\n",
    "- maybe we can compare embedding care "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
